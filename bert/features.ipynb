{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YZY/opt/anaconda3/envs/pytorch_cpu/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from mlp import mlp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import InputExample, InputFeatures\n",
    "from transformers import BertConfig, BertForSequenceClassification, BertTokenizer, BertModel\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./model were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "config=BertConfig.from_pretrained('./model')\n",
    "tokenizer=BertTokenizer.from_pretrained('./model')\n",
    "model=BertModel.from_pretrained('./model',config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_examples(lines, set_type):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    del lines[0]\n",
    "    for (i, line) in enumerate(lines):\n",
    "        guid = \"%s-%s\" % (set_type, i)\n",
    "        # label = int(line[1])\n",
    "        # CNM!!@!!\n",
    "        text_a = line[2].replace(\"YZYHUST\", ',')\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, text_b=None, label=None))\n",
    "    return examples\n",
    "\n",
    "def Load_data(tokenizer,file_path):\n",
    "    csv.field_size_limit(500 * 1024 * 1024)\n",
    "    with open(file_path, 'r') as f:\n",
    "        examples = create_examples(list(csv.reader(f)), 'predict')\n",
    "    label_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "    features = convert_examples_to_features(\n",
    "        examples,\n",
    "        tokenizer,\n",
    "        label_list=label_list,\n",
    "        max_length=256,\n",
    "        output_mode=\"classification\",\n",
    "    )\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features],\n",
    "                                 dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features],\n",
    "                                      dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features],\n",
    "                                      dtype=torch.long)\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask,\n",
    "                            all_token_type_ids)\n",
    "    return DataLoader(dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/YZY/opt/anaconda3/envs/pytorch_cpu/lib/python3.10/site-packages/transformers/data/processors/glue.py:66: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "file_path='./url/ip/ip_test.csv'\n",
    "pred_dataloader = Load_data(tokenizer,file_path=file_path)\n",
    "file=pd.read_csv(file_path)\n",
    "label=file['label']\n",
    "label.to_csv('./url/ip/test_label.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 55/55 [03:40<00:00,  4.01s/it]\n"
     ]
    }
   ],
   "source": [
    "feature_list=[]\n",
    "for batch in tqdm(pred_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                'input_ids': batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'token_type_ids':batch[2]\n",
    "            }\n",
    "            seq_outputs,pool_outputs = model(**inputs,return_dict=False)\n",
    "            feature_list.append(pool_outputs)\n",
    "features=torch.concat(feature_list,dim=0)\n",
    "torch.save(features,'./url/ip/features_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test(net):\n",
    "    net.eval()\n",
    "    labels=pd.read_csv('./features_labels_test.csv')\n",
    "    labels=labels['label']\n",
    "    length=len(labels)\n",
    "    labels=DataLoader(labels,batch_size=64)\n",
    "    features=torch.load('./features_test.pt')\n",
    "    features=DataLoader(features,batch_size=64)\n",
    "\n",
    "    sum=0\n",
    "    for feature,label in zip(features,labels):\n",
    "        output=net(feature)\n",
    "        predict=torch.argmax(output,dim=-1)\n",
    "        sum+=(predict==label).sum()\n",
    "    print(f\"accuracy on test :{sum/length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr,batch_size,epoches,over_write=False):\n",
    "    # load data\n",
    "    label_data=pd.read_csv('./features_labels.csv')\n",
    "    labels=DataLoader(label_data['label'],batch_size=batch_size)\n",
    "    features_data=torch.load('./features.pt')\n",
    "    # net and solver\n",
    "    net=mlp()\n",
    "    if os.path.exists('./classifier_model/mlp.pkl') and not over_write:\n",
    "        state_dict=torch.load('./classifier_model/mlp.pkl')\n",
    "        net.load_state_dict(state_dict=state_dict)\n",
    "    criterion=torch.nn.CrossEntropyLoss()\n",
    "    optimizer=torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    features=DataLoader(features_data,batch_size=batch_size)\n",
    "    # train\n",
    "    loss_list=[]\n",
    "    for e in range(epoches):\n",
    "        epoch_loss=[]\n",
    "        for label,feature in zip(labels,features):\n",
    "            net.train()\n",
    "            output=net(feature)\n",
    "            loss=criterion(output,label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(loss.item())\n",
    "        eval_on_test(net)\n",
    "        mean_loss=np.array(epoch_loss).mean()\n",
    "        loss_list.append(mean_loss)\n",
    "        print(f'epoch {e+1} loss: {mean_loss}')\n",
    "    loss_log=pd.DataFrame(loss_list,columns=['loss'])\n",
    "    if os.path.exists('./log/loss.csv') and not over_write:\n",
    "        f=pd.read_csv('./log/loss.csv')\n",
    "        loss_log=pd.concat([f,loss_log],axis=0)\n",
    "    loss_log.to_csv('./log/loss.csv',index=None)\n",
    "    torch.save(net.state_dict(),'./classifier_model/mlp.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test :0.8268229365348816\n",
      "epoch 1 loss: 0.1995803379783562\n",
      "accuracy on test :0.8268229365348816\n",
      "epoch 2 loss: 0.20420094601771174\n",
      "accuracy on test :0.8268229365348816\n",
      "epoch 3 loss: 0.2036771236647231\n",
      "accuracy on test :0.8268229365348816\n",
      "epoch 4 loss: 0.20452660261071287\n",
      "accuracy on test :0.8268229365348816\n",
      "epoch 5 loss: 0.2002989074777967\n",
      "accuracy on test :0.8268229365348816\n",
      "epoch 6 loss: 0.2070799386225796\n",
      "accuracy on test :0.8268229365348816\n",
      "epoch 7 loss: 0.19853777044530338\n",
      "accuracy on test :0.826171875\n",
      "epoch 8 loss: 0.20011322557305297\n",
      "accuracy on test :0.8268229365348816\n",
      "epoch 9 loss: 0.19757083319806648\n",
      "accuracy on test :0.8268229365348816\n",
      "epoch 10 loss: 0.19722782179208784\n"
     ]
    }
   ],
   "source": [
    "train(batch_size=32,lr=1e-5,epoches=10,over_write=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
